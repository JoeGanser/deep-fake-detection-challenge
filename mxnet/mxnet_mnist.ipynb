{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************\n",
    "Make sure you are using the `conda_python3` Jupyter Kernel.\n",
    "We will install the necessary libraries.\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and hosting SageMaker Models using the Apache MXNet Module API\n",
    "\n",
    "The **SageMaker Python SDK** makes it easy to train and deploy MXNet models. In this example, we train a simple neural network using the Apache MXNet [Module API](https://mxnet.apache.org/api/python/module/module.html) and the MNIST dataset. The MNIST dataset is widely used for handwritten digit classification, and consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). The task at hand is to train a model using the 60,000 training images and subsequently test its classification accuracy on the 10,000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to define a few variables that will be needed later in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MXNet and SageMaker\n",
    "_Note:  Ignore Warnings and Errors Below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 uninstall -y mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install mxnet==1.4.1 --upgrade --ignore-installed --no-cache --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mSkipping numpy as it is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/af/9f4c8faf81faee2d51ef4bd91f2e081f1a8f3ea2ae28836ac2cfd00d333f/sagemaker-1.50.0.tar.gz (291kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 59.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting boto3>=1.10.32 (from sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/f9/9798c5221d45b637ae1f42f0e0467e3bdfc3af46769b6bc7a29d93b2ecf6/boto3-1.10.46-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 73.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.9.0 (from sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.1MB 110.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.1 (from sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/ac/838c8c8a5f33a58132dd2ad2a30329f6ae1614a9f56ffb79eaaf71a9d156/protobuf-3.11.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 117.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.19.0 (from sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.1MB 105.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5 (from sagemaker)\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/55/522bb43539fed463275ee803d79851faaebe86d17e7e3dbc89870d0322b9/protobuf3-to-dict-0.1.5.tar.gz\n",
      "Collecting requests<3,>=2.20.0 (from sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 66.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting smdebug-rulesconfig==0.1.2 (from sagemaker)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/08/b0d1d30f5c1e11a2d12af3b940cf5673dc5bbb67517d815807ae855abc5e/smdebug_rulesconfig-0.1.2-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3>=1.10.32->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 74.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.14.0,>=1.13.46 (from boto3>=1.10.32->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/17/40f9fb50219e8a24fd9c09c909d26f074d4a2fc83b66065dff3770ecbebb/botocore-1.13.46-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 102.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3>=1.10.32->sagemaker)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.1->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/d3/955738b20d3832dfa3cd3d9b07e29a8162edb480bf988332f5e6e48ca444/setuptools-44.0.0-py2.py3-none-any.whl (583kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 112.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.9 (from protobuf>=3.1->sagemaker)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.20.0->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 110.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3,>=2.20.0->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 114.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests<3,>=2.20.0->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 118.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests<3,>=2.20.0->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 83.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10 (from botocore<1.14.0,>=1.13.46->boto3>=1.10.32->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 114.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.14.0,>=1.13.46->boto3>=1.10.32->sagemaker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 118.2MB/s ta 0:00:01\n",
      "\u001b[31mtensorflow 2.0.0 has requirement gast==0.2.2, but you'll have gast 0.3.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement tensorboard<2.1.0,>=2.0.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mdocker-compose 1.24.1 has requirement PyYAML<4.3,>=3.10, but you'll have pyyaml 5.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mdocker-compose 1.24.1 has requirement requests!=2.11.0,!=2.12.2,!=2.18.0,<2.21,>=2.6.1, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.13.46 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.283 has requirement PyYAML<5.2,>=3.10; python_version != \"2.6\" and python_version != \"3.3\", but you'll have pyyaml 5.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.283 has requirement rsa<=3.5.0,>=3.1.2, but you'll have rsa 4.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, docutils, six, python-dateutil, jmespath, botocore, s3transfer, boto3, numpy, setuptools, protobuf, scipy, protobuf3-to-dict, certifi, chardet, idna, requests, smdebug-rulesconfig, sagemaker\n",
      "\u001b[33m  The scripts f2py, f2py3 and f2py3.6 are installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "  Running setup.py install for protobuf3-to-dict ... \u001b[?25ldone\n",
      "\u001b[33m  The script chardetect is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[?25h  Running setup.py install for sagemaker ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed boto3-1.10.46 botocore-1.13.46 certifi-2019.11.28 chardet-3.0.4 docutils-0.15.2 idna-2.8 jmespath-0.9.4 numpy-1.18.0 protobuf-3.11.2 protobuf3-to-dict-0.1.5 python-dateutil-2.8.1 requests-2.22.0 s3transfer-0.2.1 sagemaker-1.50.0 scipy-1.4.1 setuptools-44.0.0 six-1.13.0 smdebug-rulesconfig-0.1.2 urllib3-1.25.7\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade --ignore-installed --no-cache --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install requests==2.20.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Kernel to Recognize New Dependencies Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version   \n",
      "-------------------- ----------\n",
      "absl-py              0.9.0     \n",
      "astor                0.8.1     \n",
      "astroid              1.6.6     \n",
      "attrs                19.3.0    \n",
      "awscli               1.16.76   \n",
      "awscli-cwlogs        1.4.6     \n",
      "backcall             0.1.0     \n",
      "bleach               3.1.0     \n",
      "bokeh                1.0.4     \n",
      "boto                 2.49.0    \n",
      "boto3                1.10.46   \n",
      "botocore             1.13.46   \n",
      "cachetools           4.0.0     \n",
      "certifi              2019.11.28\n",
      "chardet              3.0.4     \n",
      "cloudpickle          1.2.2     \n",
      "cmake                3.13.3    \n",
      "colorama             0.3.9     \n",
      "coremltools          2.0       \n",
      "cpplint              1.3.0     \n",
      "cycler               0.10.0    \n",
      "dask                 2.6.0     \n",
      "decorator            4.4.1     \n",
      "defusedxml           0.6.0     \n",
      "docutils             0.15.2    \n",
      "entrypoints          0.3       \n",
      "environment-kernels  1.1.1     \n",
      "future               0.17.1    \n",
      "gast                 0.3.2     \n",
      "google-auth          1.10.0    \n",
      "google-auth-oauthlib 0.4.1     \n",
      "google-pasta         0.1.8     \n",
      "graphviz             0.8.4     \n",
      "grpcio               1.26.0    \n",
      "h5py                 2.10.0    \n",
      "idna                 2.8       \n",
      "imageio              2.6.1     \n",
      "importlib-metadata   0.23      \n",
      "ipykernel            5.1.3     \n",
      "ipython              7.4.0     \n",
      "ipython-genutils     0.2.0     \n",
      "ipywidgets           7.5.1     \n",
      "isort                4.3.21    \n",
      "jedi                 0.15.1    \n",
      "Jinja2               2.10.3    \n",
      "jmespath             0.9.4     \n",
      "jsonschema           3.1.1     \n",
      "jupyter              1.0.0     \n",
      "jupyter-client       5.3.4     \n",
      "jupyter-console      6.0.0     \n",
      "jupyter-core         4.6.1     \n",
      "Keras-Applications   1.0.8     \n",
      "Keras-Preprocessing  1.1.0     \n",
      "kiwisolver           1.1.0     \n",
      "lazy-object-proxy    1.4.3     \n",
      "Markdown             3.1.1     \n",
      "MarkupSafe           1.1.1     \n",
      "matplotlib           3.1.1     \n",
      "mccabe               0.6.1     \n",
      "mistune              0.8.4     \n",
      "more-itertools       7.2.0     \n",
      "mxnet                1.5.1     \n",
      "nbconvert            5.6.1     \n",
      "nbformat             4.4.0     \n",
      "networkx             2.4       \n",
      "notebook             6.0.1     \n",
      "numpy                1.18.0    \n",
      "nvidia-ml-py         375.53.1  \n",
      "oauthlib             3.1.0     \n",
      "opencv-python        3.4.5.20  \n",
      "opt-einsum           3.1.0     \n",
      "packaging            19.2      \n",
      "pandas               0.24.2    \n",
      "pandocfilters        1.4.2     \n",
      "parso                0.5.1     \n",
      "pexpect              4.7.0     \n",
      "pickleshare          0.7.5     \n",
      "Pillow               6.2.1     \n",
      "pip                  19.0.2    \n",
      "prometheus-client    0.7.1     \n",
      "prompt-toolkit       2.0.10    \n",
      "protobuf             3.11.2    \n",
      "protobuf3-to-dict    0.1.5     \n",
      "ptyprocess           0.6.0     \n",
      "pyasn1               0.4.8     \n",
      "pyasn1-modules       0.2.7     \n",
      "pygal                2.4.0     \n",
      "Pygments             2.4.2     \n",
      "pylint               1.9.4     \n",
      "pyparsing            2.4.2     \n",
      "pyrsistent           0.15.5    \n",
      "python-dateutil      2.8.1     \n",
      "pytz                 2019.3    \n",
      "PyWavelets           1.1.1     \n",
      "PyYAML               3.13      \n",
      "pyzmq                18.1.0    \n",
      "qtconsole            4.5.5     \n",
      "requests             2.22.0    \n",
      "requests-oauthlib    1.3.0     \n",
      "rsa                  4.0       \n",
      "s3transfer           0.2.1     \n",
      "sagemaker            1.50.0    \n",
      "scikit-image         0.15.0    \n",
      "scikit-learn         0.20.2    \n",
      "scipy                1.4.1     \n",
      "Send2Trash           1.5.0     \n",
      "setuptools           44.0.0    \n",
      "simplegeneric        0.8.1     \n",
      "six                  1.13.0    \n",
      "smdebug-rulesconfig  0.1.2     \n",
      "tensorboard          2.1.0     \n",
      "tensorflow           2.0.0     \n",
      "tensorflow-estimator 2.0.1     \n",
      "termcolor            1.1.0     \n",
      "terminado            0.8.2     \n",
      "testpath             0.4.2     \n",
      "toolz                0.10.0    \n",
      "torch                1.3.1     \n",
      "torchvision          0.4.2     \n",
      "tornado              6.0.3     \n",
      "traitlets            4.3.3     \n",
      "urllib3              1.25.7    \n",
      "wcwidth              0.1.7     \n",
      "webencodings         0.5.1     \n",
      "Werkzeug             0.16.0    \n",
      "wheel                0.33.6    \n",
      "widgetsnbextension   3.5.1     \n",
      "wrapt                1.11.2    \n",
      "zipp                 0.6.0     \n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Service Execution Role and Region\n",
    "Get IAM role arn used to give training and hosting access to your data.  See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://{}/customcode/mxnet'.format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_output_path = 's3://{}/sagemaker/mxnet-mnist/training-runs'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleARN:  arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\n",
      "\n",
      "Region:  us-east-1\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print('RoleARN:  {}\\n'.format(role))\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print('Region:  {}'.format(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "The MNIST dataset has been loaded to the public S3 buckets ``sagemaker-sample-data-<REGION>`` under the prefix ``mxnet/mnist``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "train_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/train'.format(region)\n",
    "test_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/test'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the Training Data to Your Notebook Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-east-1/mxnet/mnist/train/labels.gz to data/labels.gz\n",
      "download: s3://sagemaker-sample-data-us-east-1/mxnet/mnist/train/images.gz to data/images.gz\n"
     ]
    }
   ],
   "source": [
    "!aws --region {region} s3 cp --recursive {train_data_location} {local_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.gz  labels.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls {local_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script\n",
    "\n",
    "The ``mnist_mxnet.py`` script provides all the code we need for training and hosting a SageMaker model. The script also checkpoints the model at the end of every epoch and saves the model graph, params and optimizer state in the folder `/opt/ml/checkpoints`. If the folder path does not exist then it will skip checkpointing. The script we will use is adaptated from Apache MXNet [MNIST tutorial (https://mxnet.incubator.apache.org/tutorials/python/mnist.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\r\n",
      "import gzip\r\n",
      "import json\r\n",
      "import logging\r\n",
      "import os\r\n",
      "import struct\r\n",
      "\r\n",
      "import mxnet as mx\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "\r\n",
      "def load_data(path):\r\n",
      "    with gzip.open(find_file(path, \"labels.gz\")) as flbl:\r\n",
      "        struct.unpack(\">II\", flbl.read(8))\r\n",
      "        labels = np.fromstring(flbl.read(), dtype=np.int8)\r\n",
      "    with gzip.open(find_file(path, \"images.gz\")) as fimg:\r\n",
      "        _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\r\n",
      "        images = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(labels), rows, cols)\r\n",
      "        images = images.reshape(images.shape[0], 1, 28, 28).astype(np.float32) / 255\r\n",
      "    return labels, images\r\n",
      "\r\n",
      "\r\n",
      "def find_file(root_path, file_name):\r\n",
      "    for root, dirs, files in os.walk(root_path):\r\n",
      "        if file_name in files:\r\n",
      "            return os.path.join(root, file_name)\r\n",
      "\r\n",
      "\r\n",
      "def build_graph():\r\n",
      "    data = mx.sym.var('data')\r\n",
      "    data = mx.sym.flatten(data=data)\r\n",
      "    fc1 = mx.sym.FullyConnected(data=data, num_hidden=128)\r\n",
      "    act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\r\n",
      "    fc2 = mx.sym.FullyConnected(data=act1, num_hidden=64)\r\n",
      "    act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\r\n",
      "    fc3 = mx.sym.FullyConnected(data=act2, num_hidden=10)\r\n",
      "    return mx.sym.SoftmaxOutput(data=fc3, name='softmax')\r\n",
      "\r\n",
      "\r\n",
      "def get_training_context(num_gpus):\r\n",
      "    if num_gpus:\r\n",
      "        return [mx.gpu(i) for i in range(num_gpus)]\r\n",
      "    else:\r\n",
      "        return mx.cpu()\r\n",
      "\r\n",
      "\r\n",
      "def train(batch_size, epochs, learning_rate, num_gpus, training_channel, testing_channel,\r\n",
      "          hosts, current_host, model_dir):\r\n",
      "    (train_labels, train_images) = load_data(training_channel)\r\n",
      "    (test_labels, test_images) = load_data(testing_channel)\r\n",
      "    CHECKPOINTS_DIR = '/opt/ml/checkpoints'\r\n",
      "    checkpoints_enabled = os.path.exists(CHECKPOINTS_DIR)\r\n",
      "\r\n",
      "    # Data parallel training - shard the data so each host\r\n",
      "    # only trains on a subset of the total data.\r\n",
      "    shard_size = len(train_images) // len(hosts)\r\n",
      "    for i, host in enumerate(hosts):\r\n",
      "        if host == current_host:\r\n",
      "            start = shard_size * i\r\n",
      "            end = start + shard_size\r\n",
      "            break\r\n",
      "\r\n",
      "    train_iter = mx.io.NDArrayIter(train_images[start:end], train_labels[start:end], batch_size,\r\n",
      "                                   shuffle=True)\r\n",
      "    val_iter = mx.io.NDArrayIter(test_images, test_labels, batch_size)\r\n",
      "\r\n",
      "    logging.getLogger().setLevel(logging.DEBUG)\r\n",
      "\r\n",
      "    kvstore = 'local' if len(hosts) == 1 else 'dist_sync'\r\n",
      "\r\n",
      "    mlp_model = mx.mod.Module(symbol=build_graph(),\r\n",
      "                              context=get_training_context(num_gpus))\r\n",
      "\r\n",
      "    checkpoint_callback = None\r\n",
      "    if checkpoints_enabled:\r\n",
      "        # Create a checkpoint callback that checkpoints the model params and the optimizer state after every epoch at the given path.\r\n",
      "        checkpoint_callback = mx.callback.module_checkpoint(mlp_model,\r\n",
      "                                                            CHECKPOINTS_DIR + \"/mnist\",\r\n",
      "                                                            period=1,\r\n",
      "                                                            save_optimizer_states=True)\r\n",
      "    mlp_model.fit(train_iter,\r\n",
      "                  eval_data=val_iter,\r\n",
      "                  kvstore=kvstore,\r\n",
      "                  optimizer='sgd',\r\n",
      "                  optimizer_params={'learning_rate': learning_rate},\r\n",
      "                  eval_metric='acc',\r\n",
      "                  epoch_end_callback = checkpoint_callback,\r\n",
      "                  batch_end_callback=mx.callback.Speedometer(batch_size, 100),\r\n",
      "                  num_epoch=epochs)\r\n",
      "\r\n",
      "    if current_host == hosts[0]:\r\n",
      "        save(model_dir, mlp_model)\r\n",
      "\r\n",
      "\r\n",
      "def save(model_dir, model):\r\n",
      "    model.symbol.save(os.path.join(model_dir, 'model-symbol.json'))\r\n",
      "    model.save_params(os.path.join(model_dir, 'model-0000.params'))\r\n",
      "\r\n",
      "    signature = [{'name': data_desc.name, 'shape': [dim for dim in data_desc.shape]}\r\n",
      "                 for data_desc in model.data_shapes]\r\n",
      "    with open(os.path.join(model_dir, 'model-shapes.json'), 'w') as f:\r\n",
      "        json.dump(signature, f)\r\n",
      "\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--batch-size', type=int, default=100)\r\n",
      "    parser.add_argument('--epochs', type=int, default=10)\r\n",
      "    parser.add_argument('--learning-rate', type=float, default=0.1)\r\n",
      "\r\n",
      "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\r\n",
      "\r\n",
      "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\r\n",
      "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "\r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    args = parse_args()\r\n",
      "    num_gpus = int(os.environ['SM_NUM_GPUS'])\r\n",
      "\r\n",
      "    train(args.batch_size, args.epochs, args.learning_rate, num_gpus, args.train, args.test,\r\n",
      "          args.hosts, args.current_host, args.model_dir)\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./src/mnist_mxnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add custom Python modules to the `src/requirements.txt` file.  They will automatically be installed - and made available to your training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Python dependencies go here"
     ]
    }
   ],
   "source": [
    "!cat ./src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with SageMaker's `MXNet` Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker ```MXNet``` estimator allows us to run single machine or distributed training in SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When we create the estimator, we pass in the filename of our training script, the name of our IAM execution role, and the S3 locations we defined in the setup section. We also provide a few other parameters. ``train_instance_count`` and ``train_instance_type`` determine the number and type of SageMaker instances that will be used for the training job. The ``hyperparameters`` parameter is a ``dict`` of values that will be passed to your training script -- you can see how to access these values in the ``mnist.py`` script above.\n",
    "\n",
    "For this example, we will choose one ``ml.m4.xlarge`` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet(entry_point='mnist_mxnet.py',\n",
    "                        source_dir='./src',\n",
    "                        role=role,\n",
    "                        output_path=model_output_path,\n",
    "                        code_location=custom_code_upload_location,\n",
    "                        train_instance_count=2,\n",
    "                        train_instance_type='ml.m4.xlarge',\n",
    "                        framework_version='1.4.1',\n",
    "                        py_version='py3',\n",
    "                        distributions={'parameter_server': {'enabled': True}},\n",
    "                        hyperparameters={'learning-rate': 0.1},\n",
    "                        # Assuming the logline from the MXNet training job is as follows:\n",
    "                        #    Epoch[9] Train-accuracy=0.976817\n",
    "                        #    Epoch[9] Time cost=1.200\n",
    "                        #    Epoch[9] Validation-accuracy=0.968800                    \n",
    "                        metric_definitions=[\n",
    "                            {'Name':'test:accuracy', 'Regex':'Validation-accuracy=(.*?)'},\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` the Model (Approx. 15 mins)\n",
    "\n",
    "After we've constructed our MXNet object, we can fit it using data stored in S3. Below we run SageMaker training on two input channels: **train** and **test**.\n",
    "\n",
    "During training, SageMaker makes this data stored in S3 available in the local filesystem where the mnist script is running. The ```mnist.py``` script simply loads the train and test data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  mxnet-training-2020-01-06-19-46-07-477\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator.fit({'train': train_data_location, 'test': test_data_location}, wait=False)\n",
    "\n",
    "training_job_name = mnist_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 19:46:07 Starting - Starting the training job...\n",
      "2020-01-06 19:46:09 Starting - Launching requested ML instances......\n",
      "2020-01-06 19:47:15 Starting - Preparing the instances for training......\n",
      "2020-01-06 19:48:24 Downloading - Downloading input data...\n",
      "2020-01-06 19:49:06 Training - Training image download completed. Training in progress..\u001b[34m2020-01-06 19:49:07,063 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:07,067 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:07,085 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"learning-rate\":0.1}', 'SM_USER_ENTRY_POINT': 'mnist_mxnet.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"test\",\"train\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'mnist_mxnet', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-01-06-19-46-07-477\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\"module_name\":\"mnist_mxnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_mxnet.py\"}', 'SM_USER_ARGS': '[\"--learning-rate\",\"0.1\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_HP_LEARNING-RATE': '0.1'}\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:10,110 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:10,585 sagemaker-containers INFO     Module mnist_mxnet does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:10,585 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:10,586 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:10,586 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:07,318 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:07,321 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:07,336 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"learning-rate\":0.1}', 'SM_USER_ENTRY_POINT': 'mnist_mxnet.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"test\",\"train\"]', 'SM_CURRENT_HOST': 'algo-2', 'SM_MODULE_NAME': 'mnist_mxnet', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mxnet-training-2020-01-06-19-46-07-477\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\"module_name\":\"mnist_mxnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_mxnet.py\"}', 'SM_USER_ARGS': '[\"--learning-rate\",\"0.1\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_HP_LEARNING-RATE': '0.1'}\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:10,390 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:10,807 sagemaker-containers INFO     Module mnist_mxnet does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:10,807 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:10,807 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:10,808 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/usr/local/bin/python3.6 -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mInstalling collected packages: mnist-mxnet\n",
      "  Running setup.py install for mnist-mxnet: started\n",
      "    Running setup.py install for mnist-mxnet: finished with status 'done'\u001b[0m\n",
      "\u001b[35mSuccessfully installed mnist-mxnet-1.0.0\u001b[0m\n",
      "\u001b[34mInstalling collected packages: mnist-mxnet\n",
      "  Running setup.py install for mnist-mxnet: started\n",
      "    Running setup.py install for mnist-mxnet: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed mnist-mxnet-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:12,851 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:12,882 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning-rate\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"mxnet-training-2020-01-06-19-46-07-477\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_mxnet\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_mxnet.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"learning-rate\":0.1}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_mxnet.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_mxnet\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mxnet-training-2020-01-06-19-46-07-477\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\"module_name\":\"mnist_mxnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_mxnet.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--learning-rate\",\"0.1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/usr/local/bin/python3.6 -m mnist_mxnet --learning-rate 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:13,179 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:13,196 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning-rate\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-01-06-19-46-07-477\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_mxnet\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_mxnet.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning-rate\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_mxnet.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_mxnet\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-01-06-19-46-07-477\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/customcode/mxnet/mxnet-training-2020-01-06-19-46-07-477/source/sourcedir.tar.gz\",\"module_name\":\"mnist_mxnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_mxnet.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning-rate\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m mnist_mxnet --learning-rate 0.1\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mINFO:root:Epoch[0] Batch [0-100]#011Speed: 16518.85 samples/sec#011accuracy=0.108020\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [0-100]#011Speed: 16561.07 samples/sec#011accuracy=0.114752\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [100-200]#011Speed: 17422.48 samples/sec#011accuracy=0.114900\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[0] Batch [100-200]#011Speed: 17493.28 samples/sec#011accuracy=0.110500\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[0] Train-accuracy=0.109433\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[0] Time cost=1.873\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[0] Validation-accuracy=0.113500\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Train-accuracy=0.114067\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Time cost=1.873\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Validation-accuracy=0.113500\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[1] Batch [0-100]#011Speed: 17525.17 samples/sec#011accuracy=0.108119\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [0-100]#011Speed: 17001.91 samples/sec#011accuracy=0.112871\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [100-200]#011Speed: 16465.76 samples/sec#011accuracy=0.114400\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[1] Batch [100-200]#011Speed: 16409.74 samples/sec#011accuracy=0.112900\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[1] Train-accuracy=0.152100\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[1] Time cost=1.820\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[1] Validation-accuracy=0.351100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Train-accuracy=0.154800\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Time cost=1.836\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Validation-accuracy=0.351100\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[2] Batch [0-100]#011Speed: 16348.93 samples/sec#011accuracy=0.487624\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [0-100]#011Speed: 15066.74 samples/sec#011accuracy=0.489505\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[2] Batch [100-200]#011Speed: 15709.93 samples/sec#011accuracy=0.692600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [100-200]#011Speed: 15675.05 samples/sec#011accuracy=0.676400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Train-accuracy=0.646133\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Time cost=1.958\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Validation-accuracy=0.815600\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[2] Train-accuracy=0.654233\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[2] Time cost=1.911\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[2] Validation-accuracy=0.815600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [0-100]#011Speed: 11644.01 samples/sec#011accuracy=0.814554\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[3] Batch [0-100]#011Speed: 12008.60 samples/sec#011accuracy=0.816436\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[3] Batch [100-200]#011Speed: 14237.05 samples/sec#011accuracy=0.830400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [100-200]#011Speed: 14195.48 samples/sec#011accuracy=0.830900\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[3] Train-accuracy=0.831533\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[3] Time cost=2.198\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[3] Validation-accuracy=0.846400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Train-accuracy=0.829733\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Time cost=2.221\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Validation-accuracy=0.846400\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[4] Batch [0-100]#011Speed: 13874.70 samples/sec#011accuracy=0.857426\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [0-100]#011Speed: 14292.29 samples/sec#011accuracy=0.862178\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [100-200]#011Speed: 15508.92 samples/sec#011accuracy=0.868000\u001b[0m\n",
      "\n",
      "2020-01-06 19:49:38 Uploading - Uploading generated training model\u001b[35mINFO:root:Epoch[4] Batch [100-200]#011Speed: 15547.26 samples/sec#011accuracy=0.875500\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[4] Train-accuracy=0.870867\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[4] Time cost=1.958\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[4] Validation-accuracy=0.889900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Train-accuracy=0.873033\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Time cost=1.942\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Validation-accuracy=0.889900\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[5] Batch [0-100]#011Speed: 14997.88 samples/sec#011accuracy=0.901881\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [0-100]#011Speed: 15023.33 samples/sec#011accuracy=0.900990\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[5] Batch [100-200]#011Speed: 13641.69 samples/sec#011accuracy=0.902900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [100-200]#011Speed: 13624.01 samples/sec#011accuracy=0.906700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Train-accuracy=0.907300\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Time cost=2.040\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Validation-accuracy=0.919700\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[5] Train-accuracy=0.905467\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[5] Time cost=2.038\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[5] Validation-accuracy=0.919700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [0-100]#011Speed: 16860.60 samples/sec#011accuracy=0.920099\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[6] Batch [0-100]#011Speed: 16180.29 samples/sec#011accuracy=0.920198\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[6] Batch [100-200]#011Speed: 17869.39 samples/sec#011accuracy=0.925600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [100-200]#011Speed: 17820.78 samples/sec#011accuracy=0.928300\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Train-accuracy=0.926267\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Time cost=1.875\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[6] Train-accuracy=0.924033\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[6] Time cost=1.892\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[6] Validation-accuracy=0.931400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Validation-accuracy=0.931400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [0-100]#011Speed: 17193.00 samples/sec#011accuracy=0.937030\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[7] Batch [0-100]#011Speed: 16613.72 samples/sec#011accuracy=0.938119\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[7] Batch [100-200]#011Speed: 17857.27 samples/sec#011accuracy=0.933200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [100-200]#011Speed: 17951.71 samples/sec#011accuracy=0.936400\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[7] Train-accuracy=0.937033\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[7] Time cost=1.906\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[7] Validation-accuracy=0.942000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Train-accuracy=0.938367\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Time cost=1.885\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Validation-accuracy=0.942000\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[8] Batch [0-100]#011Speed: 14913.69 samples/sec#011accuracy=0.945248\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[8] Batch [100-200]#011Speed: 19366.62 samples/sec#011accuracy=0.948700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [0-100]#011Speed: 15941.62 samples/sec#011accuracy=0.944752\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [100-200]#011Speed: 19422.14 samples/sec#011accuracy=0.944100\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[8] Train-accuracy=0.945633\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[8] Time cost=1.701\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[8] Validation-accuracy=0.949700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Train-accuracy=0.946400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Time cost=1.662\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Validation-accuracy=0.949700\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[9] Batch [0-100]#011Speed: 18526.57 samples/sec#011accuracy=0.951485\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[9] Batch [100-200]#011Speed: 16407.96 samples/sec#011accuracy=0.950700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [0-100]#011Speed: 17563.85 samples/sec#011accuracy=0.951287\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [100-200]#011Speed: 16418.70 samples/sec#011accuracy=0.952600\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[9] Train-accuracy=0.951167\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[9] Time cost=1.661\u001b[0m\n",
      "\u001b[35mINFO:root:Epoch[9] Validation-accuracy=0.953700\u001b[0m\n",
      "\u001b[35m2020-01-06 19:49:37,851 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Train-accuracy=0.952233\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Time cost=1.690\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Validation-accuracy=0.953700\u001b[0m\n",
      "\u001b[34m2020-01-06 19:49:37,871 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-06 19:49:47 Completed - Training job completed\n",
      "Training seconds: 166\n",
      "Billable seconds: 166\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1:  Perform Batch Predictions Directly in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/sagemaker/mxnet-mnist/training-runs\n"
     ]
    }
   ],
   "source": [
    "print(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 19:49:46          0 sagemaker/mxnet-mnist/training-runs/mxnet-training-2020-01-06-19-46-07-477/debug-output/training_job_end.ts\r\n",
      "2020-01-06 19:49:42     410176 sagemaker/mxnet-mnist/training-runs/mxnet-training-2020-01-06-19-46-07-477/output/model.tar.gz\r\n",
      "2019-12-31 20:44:20    7920466 sagemaker/pytorch-mnist/data/MNIST/processed/test.pt\r\n",
      "2019-12-31 20:44:19   47520466 sagemaker/pytorch-mnist/data/MNIST/processed/training.pt\r\n",
      "2019-12-31 20:44:15    7840016 sagemaker/pytorch-mnist/data/MNIST/raw/t10k-images-idx3-ubyte\r\n",
      "2019-12-31 20:44:15    1648877 sagemaker/pytorch-mnist/data/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n",
      "2019-12-31 20:44:15      10008 sagemaker/pytorch-mnist/data/MNIST/raw/t10k-labels-idx1-ubyte\r\n",
      "2019-12-31 20:44:15       4542 sagemaker/pytorch-mnist/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n",
      "2019-12-31 20:44:19   47040016 sagemaker/pytorch-mnist/data/MNIST/raw/train-images-idx3-ubyte\r\n",
      "2019-12-31 20:44:16    9912422 sagemaker/pytorch-mnist/data/MNIST/raw/train-images-idx3-ubyte.gz\r\n",
      "2019-12-31 20:44:15      60008 sagemaker/pytorch-mnist/data/MNIST/raw/train-labels-idx1-ubyte\r\n",
      "2019-12-31 20:44:15      28881 sagemaker/pytorch-mnist/data/MNIST/raw/train-labels-idx1-ubyte.gz\r\n",
      "2019-12-31 20:08:55   31360128 sagemaker/tensorflow-mnist/data/eval_data.npy\r\n",
      "2019-12-31 20:08:55      40128 sagemaker/tensorflow-mnist/data/eval_labels.npy\r\n",
      "2019-12-31 20:08:56  172480128 sagemaker/tensorflow-mnist/data/train_data.npy\r\n",
      "2019-12-31 20:08:55     220128 sagemaker/tensorflow-mnist/data/train_labels.npy\r\n",
      "2019-12-31 20:18:25      17859 sagemaker/tensorflow-mnist/training-runs/tensorflow-training-2019-12-31-20-08-57-479/model/train/events.out.tfevents.1577823480.ip-10-0-214-135.ec2.internal.112.228.v2\r\n",
      "2019-12-31 20:18:25      17859 sagemaker/tensorflow-mnist/training-runs/tensorflow-training-2019-12-31-20-08-57-479/model/train/events.out.tfevents.1577823480.ip-10-0-217-236.ec2.internal.112.228.v2\r\n",
      "2019-12-31 20:18:39    8124598 sagemaker/tensorflow-mnist/training-runs/tensorflow-training-2019-12-31-20-08-57-479/output/model.tar.gz\r\n",
      "2019-12-31 20:18:40   40636318 sagemaker/tensorflow-mnist/training-runs/tensorflow-training-2019-12-31-20-08-57-479/output/output.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws --region {region} s3 ls --recursive s3://sagemaker-us-east-1-835319576252/sagemaker # {model_output_path}/{training_job_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/400.6 KiB (2.3 MiB/s) with 1 file(s) remaining\r",
      "Completed 400.6 KiB/400.6 KiB (3.4 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-east-1-835319576252/sagemaker/mxnet-mnist/training-runs/mxnet-training-2020-01-06-19-46-07-477/output/model.tar.gz to model/model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws --region {region} s3 cp {model_output_path}/{training_job_name}/output/model.tar.gz ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-symbol.json\r\n",
      "model-0000.params\r\n",
      "model-shapes.json\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(single_loaded_img.numpy().reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2:  Create a SageMaker Endpoint and Perform REST-based Predictions\n",
    "\n",
    "After training, we use the ``MXNet estimator`` object to build and deploy an ``MXNetPredictor``. This creates a Sagemaker **Endpoint** -- a hosted prediction service that we can use to perform inference. \n",
    "\n",
    "The arguments to the ``deploy`` function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances. Here we will deploy the model to a single ``ml.m4.xlarge`` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----"
     ]
    }
   ],
   "source": [
    "predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request handling behavior of the Endpoint is determined by the ``mnist_mxnet.py`` script. In this case, the script doesn't include any request handling functions, so the Endpoint will use the default handlers provided by SageMaker. These default handlers allow us to perform inference on input data encoded as a multi-dimensional JSON array.\n",
    "\n",
    "### Making an inference request\n",
    "\n",
    "Now that our Endpoint is deployed and we have a ``predictor`` object, we can use it to classify handwritten digits.\n",
    "\n",
    "To see inference in action, draw a digit in the image box below. The pixel data from your drawing will be loaded into a ``data`` variable in this notebook. \n",
    "\n",
    "*Note: after drawing the image, you'll need to move to the next notebook cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the ``predictor`` object to classify the handwritten digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = predictor.predict(data)\n",
    "print('Raw prediction result:')\n",
    "response = response[0]\n",
    "print(response)\n",
    "\n",
    "labeled_predictions = list(zip(range(10), response))\n",
    "print('Labeled predictions: ')\n",
    "print(labeled_predictions)\n",
    "\n",
    "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "print('Most likely answer: {}'.format(labeled_predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Endpoint name: \" + predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
