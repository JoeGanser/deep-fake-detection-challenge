{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SageMaker Python SDK with TensorFlow 2.x Support (>1.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --ignore-installed --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Service Execution Role and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleARN:  arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\n",
      "\n",
      "Region:  us-east-1\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print('RoleARN:  {}\\n'.format(role))\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print('Region:  {}'.format(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = './pytorch-mnist/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./pytorch-mnist/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./pytorch-mnist/data/MNIST/raw/train-images-idx3-ubyte.gz to ./pytorch-mnist/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.3%%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./pytorch-mnist/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./pytorch-mnist/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./pytorch-mnist/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./pytorch-mnist/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./pytorch-mnist/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./pytorch-mnist/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./pytorch-mnist/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./pytorch-mnist/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./pytorch-mnist/data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./pytorch-mnist/data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "datasets.MNIST(local_data_path, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four ``.npy`` file under this prefix:\n",
    "* ``train_data.npy``\n",
    "* ``eval_data.npy``\n",
    "* ``train_labels.npy``\n",
    "* ``eval_labels.npy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pytorch-mnist/data:\r\n",
      "MNIST\r\n",
      "\r\n",
      "./pytorch-mnist/data/MNIST:\r\n",
      "processed  raw\r\n",
      "\r\n",
      "./pytorch-mnist/data/MNIST/processed:\r\n",
      "test.pt  training.pt\r\n",
      "\r\n",
      "./pytorch-mnist/data/MNIST/raw:\r\n",
      "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\r\n",
      "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\r\n",
      "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\r\n",
      "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R {local_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "data_prefix = 'sagemaker/pytorch-mnist/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-835319576252/sagemaker/pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=local_data_path, bucket=bucket, key_prefix=data_prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 17:40:20    7920421 sagemaker/pytorch-mnist/MNIST/processed/test.pt\r\n",
      "2019-12-26 17:40:14   47520423 sagemaker/pytorch-mnist/MNIST/processed/training.pt\r\n",
      "2019-12-26 17:40:13    7840016 sagemaker/pytorch-mnist/MNIST/raw/t10k-images-idx3-ubyte\r\n",
      "2019-12-26 17:40:13    1648877 sagemaker/pytorch-mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n",
      "2019-12-26 17:40:13      10008 sagemaker/pytorch-mnist/MNIST/raw/t10k-labels-idx1-ubyte\r\n",
      "2019-12-26 17:40:13       4542 sagemaker/pytorch-mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n",
      "2019-12-26 17:40:14   47040016 sagemaker/pytorch-mnist/MNIST/raw/train-images-idx3-ubyte\r\n",
      "2019-12-26 17:40:13    9912422 sagemaker/pytorch-mnist/MNIST/raw/train-images-idx3-ubyte.gz\r\n",
      "2019-12-26 17:40:13      60008 sagemaker/pytorch-mnist/MNIST/raw/train-labels-idx1-ubyte\r\n",
      "2019-12-26 17:40:13      28881 sagemaker/pytorch-mnist/MNIST/raw/train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive s3://{bucket}/{data_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/mnist_pytorch.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./src/mnist_pytorch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "mnist_estimator = PyTorch(entry_point='mnist_pytorch.py',\n",
    "                          source_dir='./src',\n",
    "                          role=sagemaker_execution_role,\n",
    "                          framework_version='1.3.1',\n",
    "                          train_instance_count=2,\n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          hyperparameters={\n",
    "                            'epochs': 6,\n",
    "                            'backend': 'gloo'\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  pytorch-training-2019-12-26-17-40-26-186\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator.fit(inputs={'training': inputs}, wait=False)\n",
    "training_job_name = mnist_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 17:40:28 Starting - Launching requested ML instances.........\n",
      "2019-12-26 17:41:34 Starting - Preparing the instances for training......\n",
      "2019-12-26 17:42:41 Downloading - Downloading input data...\n",
      "2019-12-26 17:43:05 Training - Downloading the training image......\n",
      "2019-12-26 17:44:34 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:34,616 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:34,641 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:34,713 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:34,737 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:37,757 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:38,025 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:38,025 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:38,025 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:38,025 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpohv8fct9/module_dir\u001b[0m\n",
      "\u001b[35mCollecting cli-pipeline==1.5.330\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/0f/b20b50c390730a229c817bf91a3cfdf4752bf1dee00e3f9dccf6d8a15927/cli_pipeline-1.5.330-py2.py3-none-any.whl (124kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3>=1.9.107 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.10.32)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3>=1.24.1 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.25.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=3.13 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (5.1.2)\u001b[0m\n",
      "\u001b[35mCollecting fire>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\u001b[0m\n",
      "\u001b[35mCollecting gitpython>=2.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/20/8c/4543981439d23c4ff65b2e62dddd767ebc84a8e664a9b67e840d1e2730d3/GitPython-3.0.5-py3-none-any.whl (455kB)\u001b[0m\n",
      "\u001b[35mCollecting kubernetes>=8.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/fc/2cab119f679648b348b8940de0dd744a1f0ee99c690aa2ef6072f050816c/kubernetes-10.0.1-py2.py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2>=2.10 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.10.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.14.0,>=1.13.32 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.13.32)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.21.0->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2019.11.28)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.21.0->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.21.0->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from fire>=0.1.3->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[35mCollecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\u001b[0m\n",
      "\u001b[35mCollecting gitdb2>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl (63kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.6/site-packages (from kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mCollecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:40,867 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:41,126 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:41,127 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:41,127 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:41,127 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting google-auth>=1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (42.0.2.post20191203)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2>=2.10->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.32->boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[35mCollecting smmap2>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (3.4.2)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.0.1->kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.4.8)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name, fire, termcolor\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10305 sha256=24f2226afffee70af28766f76ee54c8e560fc17ca614ee174dd9602c1448f79f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ae0nn_vz/wheels/96/bd/cb/903b7811da343f9761396f3aa7d9409defa5c4951822b99dd2\n",
      "  Building wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmptmkcuhd7/module_dir\u001b[0m\n",
      "\u001b[35m  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=8bb60e9f80fc0ad8c2eb2a6646fadb6f87d22f06487bfa58589fe44b3e88d633\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=b6bc5d7f65144c065916d7026b0972a9565433003dbe71fd7295559053852800\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name fire termcolor\u001b[0m\n",
      "\u001b[34mCollecting cli-pipeline==1.5.330\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/0f/b20b50c390730a229c817bf91a3cfdf4752bf1dee00e3f9dccf6d8a15927/cli_pipeline-1.5.330-py2.py3-none-any.whl (124kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2>=2.10 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=3.13 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (5.1.2)\u001b[0m\n",
      "\u001b[34mCollecting gitpython>=2.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/20/8c/4543981439d23c4ff65b2e62dddd767ebc84a8e664a9b67e840d1e2730d3/GitPython-3.0.5-py3-none-any.whl (455kB)\u001b[0m\n",
      "\u001b[34mCollecting fire>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\u001b[0m\n",
      "\u001b[34mCollecting kubernetes>=8.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/fc/2cab119f679648b348b8940de0dd744a1f0ee99c690aa2ef6072f050816c/kubernetes-10.0.1-py2.py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: termcolor, fire, smmap2, gitdb2, gitpython, oauthlib, requests-oauthlib, websocket-client, cachetools, pyasn1-modules, google-auth, kubernetes, cli-pipeline, default-user-module-name\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.9.107 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.10.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.24.1 in /opt/conda/lib/python3.6/site-packages (from cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2>=2.10->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.1.1)\u001b[0m\n",
      "\u001b[34mCollecting gitdb2>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl (63kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from fire>=0.1.3->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (42.0.2.post20191203)\u001b[0m\n",
      "\u001b[34mCollecting google-auth>=1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.6/site-packages (from kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mCollecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.6/site-packages (from kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.14.0,>=1.13.32 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (1.13.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.21.0->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.21.0->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mCollecting smmap2>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.32->boto3>=1.9.107->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=8.0.1->cli-pipeline==1.5.330->-r requirements.txt (line 1)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, fire, termcolor\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10300 sha256=5da224716d4b47d14bd2d39f9fa1ee965e109e4f374b760818d52d63cd5b4d44\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uq8ce6k1/wheels/34/e3/84/04f9477812548e96adfc8a9a223b39dd52e15a7768149feafb\n",
      "  Building wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mSuccessfully installed cachetools-4.0.0 cli-pipeline-1.5.330 default-user-module-name-1.0.0 fire-0.2.1 gitdb2-2.0.6 gitpython-3.0.5 google-auth-1.10.0 kubernetes-10.0.1 oauthlib-3.1.0 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 smmap2-2.0.5 termcolor-1.1.0 websocket-client-0.57.0\u001b[0m\n",
      "\u001b[35m2019-12-26 17:44:45,271 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2019-12-26-17-40-26-186\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_pytorch\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_pytorch.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"epochs\":6}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_pytorch.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_pytorch\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2019-12-26-17-40-26-186\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\",\"module_name\":\"mnist_pytorch\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_pytorch.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"6\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python mnist_pytorch.py --backend gloo --epochs 6\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=a1433e0759ab3212a9f561574577cf146132d08faaf6acd714de70cc46ef25b9\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=069a9d51bb06f675736cbf5d93e26bfea630c70ecd18612796f15a29c21206ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name fire termcolor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smmap2, gitdb2, gitpython, termcolor, fire, cachetools, pyasn1-modules, google-auth, websocket-client, oauthlib, requests-oauthlib, kubernetes, cli-pipeline, default-user-module-name\u001b[0m\n",
      "\u001b[35m<module 'cli_pipeline' from '/opt/conda/lib/python3.6/site-packages/cli_pipeline/__init__.py'>\u001b[0m\n",
      "\u001b[34mSuccessfully installed cachetools-4.0.0 cli-pipeline-1.5.330 default-user-module-name-1.0.0 fire-0.2.1 gitdb2-2.0.6 gitpython-3.0.5 google-auth-1.10.0 kubernetes-10.0.1 oauthlib-3.1.0 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 smmap2-2.0.5 termcolor-1.1.0 websocket-client-0.57.0\u001b[0m\n",
      "\u001b[34m2019-12-26 17:44:48,490 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2019-12-26-17-40-26-186\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_pytorch\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_pytorch.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":6}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_pytorch.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_pytorch\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2019-12-26-17-40-26-186\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/pytorch-training-2019-12-26-17-40-26-186/source/sourcedir.tar.gz\",\"module_name\":\"mnist_pytorch\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_pytorch.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"6\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python mnist_pytorch.py --backend gloo --epochs 6\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m<module 'cli_pipeline' from '/opt/conda/lib/python3.6/site-packages/cli_pipeline/__init__.py'>\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.110914\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.066044\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 1.214439\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 0.982412\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.908709\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 1\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.964408\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 1\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.697935\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.623431\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[34mGet test data loader\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\u001b[0m\n",
      "\u001b[35mGet test data loader\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.3300, Accuracy: 9090/10000 (91%)\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\n",
      "\u001b[0m\n",
      "\u001b[35m[2019-12-26 17:44:54.617 algo-2:60 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.3300, Accuracy: 9090/10000 (91%)\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-12-26 17:44:54.581 algo-1:60 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/30000 (21%)] Loss: 0.571392\u001b[0m\n",
      "\u001b[35m[2019-12-26 17:44:54.617 algo-2:60 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/30000 (21%)] Loss: 0.541711\u001b[0m\n",
      "\u001b[34m[2019-12-26 17:44:54.581 algo-1:60 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/30000 (43%)] Loss: 0.443480\u001b[0m\n",
      "\u001b[35m[2019-12-26 17:44:54.617 algo-2:60 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/30000 (43%)] Loss: 0.486422\u001b[0m\n",
      "\u001b[34m[2019-12-26 17:44:54.581 algo-1:60 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/30000 (64%)] Loss: 0.500146\u001b[0m\n",
      "\u001b[35m[2019-12-26 17:44:54.617 algo-2:60 INFO hook.py:325] Monitoring the collections: scalars, losses\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/30000 (64%)] Loss: 0.554598\u001b[0m\n",
      "\u001b[34m[2019-12-26 17:44:54.582 algo-1:60 INFO hook.py:325] Monitoring the collections: scalars, losses\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/30000 (85%)] Loss: 0.307704\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.110914\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/30000 (85%)] Loss: 0.293238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.066044\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1978, Accuracy: 9418/10000 (94%)\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.214439\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.964408\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1978, Accuracy: 9418/10000 (94%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/30000 (43%)] Loss: 0.982412\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.908709\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/30000 (21%)] Loss: 0.344004\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.697935\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/30000 (21%)] Loss: 0.399367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.623431\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/30000 (43%)] Loss: 0.343938\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.3300, Accuracy: 9090/10000 (91%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/30000 (43%)] Loss: 0.370680\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.3300, Accuracy: 9090/10000 (91%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/30000 (64%)] Loss: 0.504700\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/30000 (64%)] Loss: 0.449964\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/30000 (85%)] Loss: 0.390877\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.571392\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/30000 (85%)] Loss: 0.333443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.541711\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1478, Accuracy: 9545/10000 (95%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.486422\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.554598\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1478, Accuracy: 9545/10000 (95%)\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.443480\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.500146\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [6400/30000 (21%)] Loss: 0.267965\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.307704\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/30000 (21%)] Loss: 0.353790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.293238\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/30000 (43%)] Loss: 0.265868\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1978, Accuracy: 9418/10000 (94%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [12800/30000 (43%)] Loss: 0.302168\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1978, Accuracy: 9418/10000 (94%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [19200/30000 (64%)] Loss: 0.392489\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/30000 (64%)] Loss: 0.381731\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/30000 (85%)] Loss: 0.324791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.399367\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [25600/30000 (85%)] Loss: 0.283052\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.344004\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1201, Accuracy: 9626/10000 (96%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.370680\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.504700\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1201, Accuracy: 9626/10000 (96%)\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.343938\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.449964\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [6400/30000 (21%)] Loss: 0.269720\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.390877\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/30000 (21%)] Loss: 0.286095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.333443\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/30000 (43%)] Loss: 0.169939\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1478, Accuracy: 9545/10000 (95%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [12800/30000 (43%)] Loss: 0.356303\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1478, Accuracy: 9545/10000 (95%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [19200/30000 (64%)] Loss: 0.331643\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/30000 (64%)] Loss: 0.418289\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/30000 (85%)] Loss: 0.272007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.353790\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [25600/30000 (85%)] Loss: 0.323437\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.267965\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1042, Accuracy: 9682/10000 (97%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.265868\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.381731\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1042, Accuracy: 9682/10000 (97%)\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.302168\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.392489\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [6400/30000 (21%)] Loss: 0.225308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.283052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/30000 (21%)] Loss: 0.192137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.324791\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/30000 (43%)] Loss: 0.290625\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1201, Accuracy: 9626/10000 (96%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [12800/30000 (43%)] Loss: 0.216728\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1201, Accuracy: 9626/10000 (96%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [19200/30000 (64%)] Loss: 0.324668\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/30000 (64%)] Loss: 0.392888\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/30000 (85%)] Loss: 0.237660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.286095\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [25600/30000 (85%)] Loss: 0.240622\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.269720\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0938, Accuracy: 9701/10000 (97%)\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.356303\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.331643\u001b[0m\n",
      "\u001b[35mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.323437\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1042, Accuracy: 9682/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.225308\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.216728\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.324668\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.240622\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0938, Accuracy: 9701/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model.\u001b[0m\n",
      "\u001b[35m[2019-12-26 17:45:53.016 algo-2:60 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[35m2019-12-26 17:45:53,432 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-12-26 17:46:03 Uploading - Uploading generated training model\n",
      "2019-12-26 17:46:03 Completed - Training job completed\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0938, Accuracy: 9701/10000 (97%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.169939\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.418289\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.272007\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1042, Accuracy: 9682/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.192137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.290625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.392888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.237660\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0938, Accuracy: 9701/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m[2019-12-26 17:45:53.078 algo-1:60 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2019-12-26 17:45:53,447 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 404\n",
      "Billable seconds: 404\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator = PyTorch.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1:  Perform Batch Predictions Directly in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2:  Perform Batch Predictions with a Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 3:  Create a SageMaker Endpoint and Perform REST-based Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Create endpoint\n",
    "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "As mentioned above we have implementation of `model_fn` in the `mnist.py` script that is required. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`. Here we will deploy the model to a single ```ml.m4.xlarge``` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.c5.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which we can then pass to the `predictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = np.array([data], dtype=np.float32)\n",
    "response = predictor.predict(image)\n",
    "prediction = response.argmax(axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
